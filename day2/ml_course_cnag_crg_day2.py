# -*- coding: utf-8 -*-
#%% [markdown]
"""ML_course_CNAG-CRG_day2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I2nf-XxQKdv5DfOw8S1rXg3vwtvGyz75

# Supervised learning

Supervised learning aims to predict labels of data using annotated examples. Supervised algorithms learn patterns on "tagged" data for later predicting the outcome of data which the model has not seen before. 

There are two branches in supervised learning:


*   **Classification:** learns and predicts discrete labels on the data (e.g. Diabetic/Healthy, Dead cell/Live cell...)  
*   **Regression:** predicts a continuous value based on the a set of explanatory variables (e.g. weight based on the height or age based on methylation pattern).

Two important concepts in supervised learning are **response variable** and the **predictive variables**. Supervised models aim to predict the response variable using the values from the predictor variables.

Let's look at examples:

The iris dataset contains information about 3 species of plants.
"""

#%%
# import packages
import numpy as np
import pandas as pd
from sklearn import datasets

# load dataset
iris_dataset = datasets.load_iris()

# create data frame with predictive and variables
iris_table = pd.DataFrame(data = iris_dataset['data'], 
                          columns = iris_dataset['feature_names'])

# add response variable to the table
iris_table['species'] = pd.Categorical.from_codes(iris_dataset.target, iris_dataset.target_names)

# show first five rows of the dataframe
iris_table.head()

#%% [markdown]
"""Another example is the diabetes dataset."""

#%%
# load dataset
diabetes_dataset = datasets.load_diabetes()

# create data frame with predictive variables
diabetes_table = pd.DataFrame(data = diabetes_dataset['data'],
                              columns = diabetes_dataset['feature_names'])

# add response variable to the table
diabetes_table['disease_progression'] = pd.Series(data = diabetes_dataset['target'])

# show first five rows of the dataframe
diabetes_table.head()

#%% [markdown]
"""# Regression

Regression algorithms learn the relationship between the predictor and the response variable. The output is a numeric value.

## Simple linear regression

Simple linear regression tries to find the linear relationship between one predictor variable and the response variable. In other words it tries to find the line which best fits the training data. It follows the formula:

<br>

 $$ 
  \hat y = \beta_0 + \beta_1  x 
 $$

<br>

* $\hat y$ = predicted value
* $x$ = predictor variable
* $\beta_1$ = model coefficient (slope of the line in simple linear regression)
* $\beta_0$ = bias term (intercept in simple linear regression)

Let's see a simple example with the iris dataset:
"""

#%%
# Commented out IPython magic to ensure Python compatibility.
import seaborn as sns
# %matplotlib inline

# plot the petal width v.s. the petal length
sns.relplot(data = iris_table,
            x = iris_table.columns[2], y = iris_table.columns[3])

#%% [markdown]
"""<br>

Fit a linear regression model to the data.
"""

#%%
from sklearn import linear_model

# create a linear regression object
regres_iris = linear_model.LinearRegression()

# create new arrays for the predictive and response variables
pet_wid = iris_dataset['data'][:, np.newaxis, 3]
pet_len = iris_dataset['data'][:, np.newaxis, 2]

# fit linear regression model to the data
regres_iris.fit(pet_len, pet_wid)

# extract slope 
m = float(regres_iris.coef_)

# extract intercept
n = float(regres_iris.intercept_)

print('slope = {} and intercept = {}'.format(m, n))

# %% [markdown]
"""<br>
Plot fitted line
"""

import matplotlib.pyplot as plt
plt.plot(iris_dataset['data'][:,2], iris_dataset['data'][:,3], 'o')
plt.plot(iris_dataset['data'][:,2], m*iris_dataset['data'][:,2] + n)
plt.xlabel("petal length (cm)")
plt.ylabel("petal width (cm)")

# %% [markdown]
"""<br>

The linear model follows the equation shown above:

<br>

 $$ 
  \hat y = \beta_0 + \beta_1  x 
 $$

<br>

* $ \beta_1 $ = 0.42
* $ \beta_0 $ = -0.36

<br>

$$ \hat y = 0.42 -0.36 x $$

<br>

How are $ \beta_0 $ and $ \beta_1 $ estimated? In other words how is the line which fits the data best found?

In regression algorithms the aim is to find the model which minimises the loss function. Choosing the apropriate loss function is key in machine learning. In the case of simple linear regression the aim is to find the line which minimises the difference between the predicted values and the true values. 

Let's look at it in this image: 

<br>

![image.png](https://littleml.files.wordpress.com/2019/03/residuals-1.png)

<br>

The difference between the predicted value and the true value is called residual (doted lines). To account for the fact that predictions can be higher and lower than the true value (this is residuals can be positive or negative), a linear model aims to minimise the Residual Sum of Squares (RSS):

<br>

$$ RSS(\beta) = \sum_{i=1}^{N} (\hat y - y)^2 $$

<br>

* $\beta$ = {$\beta_0, \beta_1$... $\beta_n$}
* $N$ = number of data points
* $\hat y$ = predicted value
* $y$ = true value

<br>


For linear regression models, there is a closed-formed solution to compute the best coefficient values using the [Normal equation](https://towardsdatascience.com/performing-linear-regression-using-the-normal-equation-6372ed3c57).

If our model is unbiased we would expect the residuals to distribute randomly as seen in the plot below:




"""
#%%
# create a residuals plot
sns.residplot(iris_dataset['data'][:,2],iris_dataset['data'][:,3])


# %% [markdown]
"""## Evaluate the model

There are different metrics to evaluate the performance of linear regression models.

### Mean Absolute Error (MAE)

The **Mean absolute error** is perhaps the simplest metric. It computes the average absolute difference between predictions and actual values. It therefore follows the formula:

$$ MAE(\beta) = {\frac{1}{N}}\sum_{i=1}^{N} |\hat y - y| $$ 

* $\beta$ = {$\beta_0, \beta_1$... $\beta_n$}
* $N$ = number of data points
* $\hat y$ = predicted value
* $y$ = true value


<br>


In the case of MAE all residuals are weighted equally. For this reason it is not optimal when outliers are present.

The interpretation of this metric relatively straight forward. It indicates the average error of the model (regardless of direction). For example, if a linear regression model which predicts the height (cm) based on the weight (kg) has MAE = 3, it indicates that the model predictions have an average of error ±3 cm.

<br>

### Root Mean Square Error (RMSE)

The **Root mean square error**  is another common metric to evaluate the performance of machine learning algorithms. It follows the formula:

<br>

$$ RMSE(\beta) =  \sqrt {\frac{1}{N}\sum_{i=1}^{N}(\hat y - y)^2} $$


* $\beta$ = {$\beta_0, \beta_1$... $\beta_n$}
* $N$ = number of data points
* $\hat y$ = predicted value
* $y$ = true value

<br>

Since the differences between predicted and true values are squared, the importance of errors increases quadratically on their value. This means that predictions which are very far from the true values are penalised by RMSE.

### Coeficient of determination ($ R^2$)

There are other metrics which evaluate how good the model fits data. The coefficient of determination or R-squared ($R^2$) is one example. It measures the proportion of total variance of the response variable explained by model. The higher the value the better the model fits the data. It follows the equation:

<br>

$$ R^2 = 1 - \frac {SS_{res}}{SS_{tot}} = 1 - \frac {\sum_{i=1}^{N} (\hat y - y)^2}{\sum_{i=1}^{N}(y - \bar y)^2} $$

* $ \hat y $ =  predicted value
* $ y $ = true value
* $ \bar y $ = mean of response variable
* $ N $ = number of data points

<br>

## Build linear model

Let's now fit a simple linear model to the iris dataset and make predictions. In this case we are interested in predicting the petal width (response variable) using the petal length as predictor variable.


The process has the following steps:

1. Split dataset into training and test set
2. Fit linear model to the training data
3. Apply the model to test set.
4. Evaluate the model performance (MAE, RMSE and $R^2$)
"""

#%%
# split dataset into training and test set

# set a random seed so that the randomly sampled rows are always the same
import random
random.seed(20)

# sample 20 row numbers to use as test set
rows_test = random.sample(range(0,pet_len.shape[0]),20)

# get the remaining row numbers as training set
rows_train = np.setxor1d(rows_test, list(range(0, pet_len.shape[0])))

# split predictor variable into test and training set
pet_len_train = np.delete(pet_len, rows_test)[:,np.newaxis]
pet_len_test = np.delete(pet_len, rows_train)[:,np.newaxis]

# split response variable into test and training set
pet_wid_train = np.delete(pet_wid, rows_test)[:, np.newaxis]
pet_wid_test = np.delete(pet_wid, rows_train)[:,np.newaxis]

print("Training set has {} observations and test set has {}".format(pet_len_train.shape[0], pet_len_test.shape[0]))

#%%
# fit a line to the training set

# create a linear regression object
regres_pet = linear_model.LinearRegression()

# fit a linear model to the training data
regres_pet.fit(pet_len_train, pet_wid_train)

# plot the length v.s. width the the fitted line
plt.plot(pet_len_train , pet_wid_train, 'o')
plt.plot(pet_len_train, regres_pet.predict(pet_len_train))
plt.xlabel("petal length (cm)")
plt.ylabel("petal width (cm)")

# make predictions on the test set
pet_wid_pred = regres_pet.predict(pet_len_test)

# create data frame with predicted and true values
pred_table_wid = pd.DataFrame.from_dict({'Predicted_width': pet_wid_pred.flatten(), 'True_width': pet_wid_test.flatten()})

#%%
# print data frame 
pred_table_wid

#%%
# Make scatter plot of predicted vs true sepal width
plt.plot(pet_wid_test, pet_wid_pred, 'o')
plt.plot(pet_wid_test, pet_wid_test)
plt.xlabel('True_width')
plt.ylabel('Predicted width')

#%%
# make residuals plot
sns.residplot(pet_wid_test, pet_wid_pred)

#%%
# evaluate the performance of the model

# import metrics package
from sklearn import metrics

# compute MAE
petal_mae = metrics.mean_absolute_error(pet_wid_pred, pet_wid_test)

# get RMSE
petal_rmse = metrics.mean_squared_error(pet_wid_pred, pet_wid_test,
                           squared = False)

print("The model has a MAE = {} and a RMSE = {}".format(petal_mae, petal_rmse))

#%%
# get squared R 
metrics.r2_score(pet_wid_pred, pet_wid_test)

# %% [markdown]
"""## Exercise 

Let's now do an exercise. The iris dataset contains two additional variables sepal length and sepal width. How well can we predict the length of sepals using the length of petals? 

1. Fit a linear model to predict sepal length uisng the petal length 
2. Predict the sepal length of a test set using the model
3. Compute the MAE, RMSE and $R^2$.

"""

#%%
# the column we are interested now are 0 and 2
iris_table

#%%
# the relationship seems different for the setosa species
sns.relplot(data = iris_table, x = iris_table.columns[0], y = iris_table.columns[2], hue="species")

#%%
# exclude setosa species for the downstream analysis

# get sepal length for versicolor and virginica species
sep_len = iris_table[iris_table.species != "setosa"].iloc[:, 0][:, np.newaxis]

# get petal length for versicolor and virginica species
pet_len_filt = iris_table[iris_table.species != "setosa"].iloc[:,2][:, np.newaxis]

#%% [markdown]
"""### Solution"""

#%%
random.seed(10)

# sample 5 row numbers to use as test set
rows_test = random.sample(range(0, sep_len.shape[0]),20)

# get the remaining row numbers as training set
rows_train = np.setxor1d(rows_test, list(range(0, sep_len.shape[0])))

# split predictor variable into test and training set
sep_len_train = np.delete(sep_len, rows_test)[:,np.newaxis]
sep_len_test = np.delete(sep_len, rows_train)[:,np.newaxis]

# split response variable into test and training set 
pet_len_train = np.delete(pet_len_filt, rows_test)[:, np.newaxis]
pet_len_test = np.delete(pet_len_filt, rows_train)[:, np.newaxis]

# create a linear regression object
regres_sep = linear_model.LinearRegression()

# fit a linear model to the training data
regres_sep.fit(pet_len_train, sep_len_train)

# predict values of test set
sep_len_pred = regres_sep.predict(pet_len_test)

#%%
# plot predicted values v.s. true values
plt.plot(sep_len_test, sep_len_pred, 'o')
plt.plot(sep_len_test, sep_len_test, "-.")

#%%
# plot fitted line to train data
plt.plot(pet_len_train, sep_len_train, 'o')
plt.plot(pet_len_train, regres_sep.predict(pet_len_train))

#%%
# get MAE 
metrics.mean_absolute_error(sep_len_test, sep_len_pred)

#%%
# get RMSE 
metrics.mean_squared_error(sep_len_test, sep_len_pred, squared = False)

#%%
# get squared R
metrics.r2_score(sep_len_test, sep_len_pred)

# %% [markdown]
"""## Multiple linear regression

In many cases we have more than one predictor variable describing the response variable. The same linear regression model can be applied in such cases. In that case there is one additional parameter per additional predictor variable. With these multidimensional data instead of finding the line which best fits the data, the plane (2 predictor variables) or hyperplane (more than 2 predictor variables) is sought.

<br>

$$ \hat y = \beta_0 + \beta_1x_{1} + \beta_2x_{2} ... \beta_nx_{n} $$ 

<br>

* $\hat y $ = predicted value 
* $ \beta_i $ = parameters or coefficients of the model
* $ x_{i} $ = predictor variable  

<br> 

Let's fit a multiple linear regression model to the diabetes dataset. The response variable in this case is termed disase progression.

"""

#%%
# make histogram of disease progression values
sns.histplot(data = diabetes_table, x = 'disease_progression')

from sklearn.model_selection import train_test_split

# split dataset into test and training set
# scikit learn includes a function to do so
# we use 80% of the data for training and 20 % as test set
diab_train, diab_test, progr_train, progr_test = train_test_split(diabetes_dataset['data'], diabetes_dataset['target'],
                                                                  test_size = 0.2, random_state = 42)

# create a linear regression object
diab_lin_model = linear_model.LinearRegression()

# fit the linear model to the training data
diab_lin_model = diab_lin_model.fit(diab_train, progr_train)

# predict values of test set
diab_pred = diab_lin_model.predict(diab_test)

# make data frame with predicted and true values
pred_table_diab = pd.DataFrame.from_dict({'Predicted_progression': diab_pred.flatten(), 'True_progression': progr_test.flatten()})

#%%
# print data frame 
pred_table_diab

#%%
# plot predicted values vs real values
plt.plot(progr_test, diab_pred, 'o')
plt.plot(progr_test, progr_test, "--")
plt.xlabel("True progression (a.u.)")
plt.ylabel("Predicted progression(a.u.)")

#%%
# get MAE
diab_mae = metrics.mean_absolute_error(progr_test, diab_pred)

print('The MAE of the model is {}'.format(diab_mae))

#%%
# get RMSE
diab_rmse = metrics.mean_squared_error(progr_test, diab_pred,
                                       squared = False)

print('The RMSE of the model is {}'.format(diab_rmse))

#%%
# get squared R 
diab_rsqr = metrics.r2_score(progr_test, diab_pred)

print('The R^2 of the model is {}'.format(diab_rsqr))

#%% [markdown]
"""## Polynomial regression

There are occasions in which the relationship between the dependent and independent variables is not linear. In such cases a linear regression model would not fit the data well. 

Let's see an example:
"""

#%%
# load boston dataset
boston_dataset = datasets.load_boston()

# get dependent and independent variables as arrays
y_nox = boston_dataset['data'][:, np.newaxis ,4]
x_dis = boston_dataset['data'][:, np.newaxis ,7]

# make dataframe with dependent and independent variable
boston_table = pd.DataFrame.from_dict({'y_nox': y_nox.flatten(),
                                       'x_dis': x_dis.flatten()})

# make scatter plot
sns.relplot(data = boston_table, x = boston_table.columns[1], y = boston_table.columns[0])

#%%
bost_lin_model = linear_model.LinearRegression().fit(x_dis, y_nox)

# compute squared R
rsqr_bos_lin = metrics.r2_score(y_nox, bost_lin_model.predict(x_dis))

print('The R^2 of the model is {}'.format(rsqr_bos_lin))

#%%
# get RMSE
rmse_bos_lin = metrics.mean_squared_error(y_nox, bost_lin_model.predict(x_dis),
                                         squared = False)

print('The RMSE of the model is {}'.format(rmse_bos_lin))

#%%
# make scatter plot with fitted line
plt.plot(x_dis, y_nox, 'o')
plt.plot(x_dis, bost_lin_model.predict(x_dis), 'r')

#%%
# make residuals plot
sns.residplot(x = x_dis.flatten(), y = y_nox.flatten())

# %% [markdown]
"""The following steps are done when carrying out polynomial regression:

1. Generate polynomial features from original features.
2. Fit a linear regression model to the extended feature set.
3. Evalute the fit of the model

<br>
"""

#%%
from sklearn.preprocessing import PolynomialFeatures

# generate new features (second degree polynomial)
poly_features = PolynomialFeatures(degree = 2, include_bias = False)
x_dis_poly = poly_features.fit_transform(x_dis)

x_dis_poly[0]

#%%
# fit linear model to extended data
bost_poly_model = linear_model.LinearRegression().fit(x_dis_poly, y_nox)

#%%
# compute squared R
rsqr_bos_poly = metrics.r2_score(y_nox, bost_poly_model.predict(x_dis_poly))

print('The R^2 of the model is {}'.format(rsqr_bos_poly))

#%%
# get RMSE
rmse_bos_poly = metrics.mean_squared_error(y_nox, bost_poly_model.predict(x_dis_poly),
                                         squared = False)

print('The RMSE of the model is {}'.format(rmse_bos_poly))

#%%
# make scatter plot with predicted and real values
plt.plot(x_dis, y_nox, 'o')
plt.plot(x_dis, bost_poly_model.predict(x_dis_poly), 'o')

#%%
# get coefficients from the model 
bost_poly_model.coef_

# %% [markdown]
"""Since we used a polynomial of degree 2, out model follows the equation:

<br>

$$ \hat y = \beta_0 + \beta_1x_1 + \beta_2x_1^2  $$

## Final exercise
To end this section let's explore the relationship between methylation and age. In the dataset below we can find the methylation levels for 53 CpGs from 100 individuals as well as the corresponding age. The dataset was obtained from [Daunay et al., 2019](https://www.nature.com/articles/s41598-019-45197-w#Sec14). Methylation levels were measured from blood samples using pyrosequencing. 

Can we predict the age of the individuals based on the methylation levels using linear regression? How accurate are our predictions?
"""

#%%
# load dataset 
age_table = pd.read_csv("methylation_data.csv")

# transform it into an array
meth_array = age_table.to_numpy()

#%% [markdown]
"""### Solution"""

#%%
# split dataset into test and training set
meth_train, meth_test, age_train, age_test = train_test_split(meth_array[:,1:99], meth_array[:, 0],
                                                              test_size = 0.2, random_state = 42)

# create a linear regression object
age_lin_model = linear_model.LinearRegression()

# fit the linear model to the training data
age_lin_model = age_lin_model.fit(meth_train, age_train)

# predict values of test set
age_pred = age_lin_model.predict(meth_test)

# make data frame with predicted and true values
pred_table_age = pd.DataFrame.from_dict({'Predicted_age': age_pred.flatten(), 'Chronological_age': age_test.flatten()})

# print data frame 
pred_table_age

#%%
# plot predicted age v.s. chronological age 
plt.plot(age_test ,age_pred, 'o')
plt.plot(age_test, age_test)
plt.xlabel("Chronological age (years)")
plt.ylabel("Predicted age (years)")

#%%
# compute MAE 
age_mae = metrics.mean_absolute_error(age_test, age_pred)

# compute RMSE 
age_rmse = metrics.mean_absolute_error(age_test, age_pred)

# compute squared R
age_sqrtr = metrics.r2_score(age_test, age_pred)

print("The model predicts age with accuracy ±{} years".format(round(age_mae, 2)))

# %% [markdown]
"""# Classification

## Decission Tree

A decision tree is a supervised learning algorithm that is perfect for classification problems, as it’s able to order classes on a precise level. It works like a flow chart, separating data points into two similar categories at a time from the “tree trunk” to “branches,” to “leaves,” where the categories become more finitely similar. This creates categories within categories, allowing for organic classification with limited human supervision.

schematic           |  example
:-------------------------:|:-------------------------:
![image.png](https://miro.medium.com/max/700/1*SKFU4V4qWNDU8sXgq36ncw.png)|![image.png](https://d33wubrfki0l68.cloudfront.net/cb281b80c41c9e76eb327e26ed5e0e6e5f05fc7f/31de9/static/b42ef5448b11ec2f2ec20ca7f97cbb3c/9cda9/decision-tree-sports.png)

**Underlying principle of the algorithm**

It recursively splits the training set till it no longer
finds a split which reduces the impurity

first split|count missclassification
:-------------------------:|:-------------------------:
![image.png](https://miro.medium.com/max/501/0*UvBgln5Di3yeLZHI.png)|![image.png](https://miro.medium.com/max/500/0*NFb_YzdigP99hgjC.png)

Find the missclassification for every split on X1|This is the best split
:-------------------------:|:-------------------------:
![image.png](https://miro.medium.com/max/514/0*NLvKQda-yuqa4Wee.png)|![image.png](https://miro.medium.com/max/491/0*2Hu0AMYGLXqgEMhV.png)

Find the next split | Finished
:-------------------------:|:-------------------------:
![image.png](https://miro.medium.com/max/510/0*zV-l2CROY1ZP_3qA.png)|![image.png](https://miro.medium.com/max/497/0*uGZjGt6N7BRxb46Q.png)

Resulting tree
![image.png](https://miro.medium.com/max/504/0*Z66SWGnP7OqSLuyC.png)


source:
https://towardsdatascience.com/learn-how-decision-trees-are-grown-22bc3d22fb51

Above our cost function we minimized was the number of missclassification
Two other common ones are gini impurity and information gain (entropy)

![image.png](https://miro.medium.com/max/1204/0*Nsuwaq2Padpbdz9Y.png)

It is out of the scope of this course to explain the math behind it
but a takehome message key difference between gini and entropy would be:

Gini: the feature with a lower Gini index is chosen for a split
Entropy: The feature with the largest information gain should is used as the root node

Further read:
https://blog.clairvoyantsoft.com/entropy-information-gain-and-gini-index-the-crux-of-a-decision-tree-99d0cdc699f4
https://towardsdatascience.com/gini-index-vs-information-entropy-7a7e4fed3fcb

##### Load Libraries
"""

#%%
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.datasets import load_iris

from sklearn.tree import DecisionTreeClassifier,plot_tree

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.feature_selection import SelectFromModel

from sklearn.metrics import accuracy_score,confusion_matrix,classification_report

#%% [markdown]
"""##### Exploring the iris dataset"""

#%%
iris=load_iris()

# %% [markdown]
"""The iris dataset contains the following data

* 50 samples of 3 different species of iris (150 samples total)
* Measurements: sepal length, sepal width, petal length, petal width

![link text](https://raw.githubusercontent.com/ritchieng/machine-learning-dataschool/master/images/03_iris.png)
"""

#%%
print ("iris.data");print()
print("type");print(type(iris.data)); print()
print("shape");print(iris.data.shape);print()
print("first 5 rows of the iris dataset:");print()
iris.data[:5]

#%% [markdown]
"""Machine learning terminology

*  row=an observation (also known as: sample,record)
*  column= feature (also known as: predictor, attribute, independent variable)

"""

#%%
print("features:"); print (iris.feature_names);print()

print("iris.target")
print("integers representing the species of each observation");print()
print("shape");print(iris.target.shape);print()
print(iris.target); print()

print("iris.target_names")
print("encoding scheme for species: 0 = setosa, 1 = versicolor, 2 = virginica")
print(iris.target_names)

#%%
full_df = pd.DataFrame(iris.data, columns=iris.feature_names)
full_df["species"] = iris.target

code2species={0:"setosa",1:"versicolor",2:"virginica"}
full_df["species_name"] = full_df["species"].map(code2species)
full_df

#%%
sns.pairplot(
    full_df.drop(["species"],axis=1), 
    hue='species_name',markers=["o", "s", "D"])
plt.show()

# %% [markdown]
"""You see that iris-setosa (dot) is easily identifiable by petal length and petal width, while the other two species are much more difficult to distinguish

#### **Preprocess Data**

So above would now be a really nice "tidy" format which we could use for in depth explanatory analysis. However, since we want to use Machine Learning (i.e. scikit learn) we have to make sure that we fullfill all requirements:

Features and target: 
1. are separate objects
2. should be numeric
3. should be NumPy arrays
4. should have compatible shapes

Luckily in this dataset all the requirements are already fullfilled
So we only need to split the data in training and test set
"""

#%%
# Split dataset into training 80% and test (20%) set
test_size=0.2
x_train, x_test, y_train, y_test = train_test_split(
    iris.data, iris.target, test_size=test_size)

for t in [("training",x_train),("test",x_test)]:
  print(f"observations in the {t[0]} set:{len(t[1])}")

#%% [markdown]
"""#### Create Decission Tree classifier"""

#%%
dct_clf = DecisionTreeClassifier(random_state=123456)
dct_clf.fit(x_train, y_train)

# %% [markdown]
"""Tada! We have done it! We have officially trained our decission tree classifier! Now let’s play with it

#### Apply Classifier to Test Data
"""

#%%
# Apply the Classifier we trained to the test data (which, remember, it has never seen before)
targets_predicted=dct_clf.predict(x_test)
targets_predicted

# %% [markdown]
"""What are you looking at above? Remember that we coded each of the three species of plant as 0, 1, or 2. What the list of numbers above is showing you is what species our model predicts each plant is based on the the sepal length, sepal width, petal length, and petal width. 

How confident is the classifier about each plant? We can see that too.


"""

#%%
# View the predicted probabilities of the first 5 observations
dct_clf.predict_proba(x_test)[0:5]

# %% [markdown]
"""There are three species of plant, thus [ 1. , 0. , 0. ] tells us that the classifier is certain that the plant is the first class.

If you would have for example a row with [ 0., 0.9, 0.1. ]the classifier gives a 90% probability the plant belongs to the 2nd class and a 10% probability the plant belongs to the 3rd class. Because 90 is greater than 10, the classifier would predict the plant is the 2nd class.

"""

#%%
def appply_clf(clf,dataset,x_test,y_test):

  df=pd.DataFrame(x_test, columns=dataset.feature_names)
  df["target"]=y_test
  df["target_predicted"]=clf.predict(x_test)
  return df

test_df=appply_clf(dct_clf,iris,x_test,y_test)
test_df.head()

# %% [markdown]
"""That looks pretty good! At least for the first five observations.
So lets do a quick evaluation

"""

print(f'accuracy:{accuracy_score(y_test, dct_clf.predict(x_test))}')

# %% [markdown]
"""Before we evaluate the classifier further lets explore and try to understand the used classifier

#### Explore the classifier

View Feature Importance

This is in my opinion the most powerful parts of tree based classification models. The importance of each feature can not only help you to understand better how the algorithm comes to its decision but also might give you insights into the underlying biology
"""

#%%
def get_feature_importance(clf,dataset):
  feature_imp = pd.Series(
      clf.feature_importances_,index=dataset.feature_names
      ).sort_values(ascending=False)

  sns.barplot(x=feature_imp, y=feature_imp.index)
  plt.xlabel('Feature Importance Score')
  plt.ylabel('Features')
  plt.title("Visualizing Important Features")
  plt.legend()
  plt.show()

get_feature_importance(dct_clf,iris)

# %% [markdown]
"""As you can see the library has identified that petal length & width are the most important features in terms of making the split.

Lets start with plotting the tree
"""

#%%
plt.figure(figsize=(4,4), dpi=300)
plot_tree(dct_clf, feature_names=iris.feature_names, 
          class_names=iris.target_names, filled=True)
plt.show()

# %% [markdown]
"""In this visualization, each color signifies a iris species. The shade of the color is indicative of the purity

#### Improve our classifier

That is a quite "complex" tree lets see if we can simplify it?

`DecisionTreeClassifier` has a lot of configuration options.

|Option |Description|
--- | --- 
|max_depth|How many split will my tree be allowed to do?|
|criterion|loss function to train my tree (entropy/gini)|
|max_features|The number of features to consider when looking for the best split|
|max_leaf_nodes|Limit of how many leaves each split can produce|
|min_samples_leaf|The minimum number of samples required to produce a leaf|
|min_samples_split|Number of samples required in the leaf before splitting|

So it would be quite tedious to test them all on your own:
--> Meet GridSearch
"""

#%%
dct_clf2 = DecisionTreeClassifier(random_state=123456)
params = {
    "max_depth"         : [1,2,3,4,5,10],
    "criterion"         : ['entropy', 'gini'],
    "max_features"      : [0.5, 1],
    "max_leaf_nodes"    : [2,3,5],
    'min_samples_leaf'  : [2,3,5],
    'min_samples_split' : [2,3,5]
}
gs = GridSearchCV(dct_clf2, params, cv=3)
gs.fit(x_train, y_train)
print(gs.best_params_)
print()
dct_clf2=gs.best_estimator_
dct_clf2.fit(x_train,y_train)
dct_clf2

# %% [markdown]
"""So lets see how our tree has changed"""

#%%
plt.figure(figsize=(4,4), dpi=300)
plot_tree(dct_clf2, feature_names=iris.feature_names, 
          class_names=iris.target_names, filled=True)
plt.show()

# %% [markdown]
"""Indeed much simpler as our old tree but before we see how this impacts our performance. So again a quick evaluation"""

#%%
print("accurcacy:")
for t in [("dct_clf",dct_clf),("dct_clf2",dct_clf2)]:
  print(f'{t[0]}:{accuracy_score(y_test, t[1].predict(x_test))}')

# %% [markdown]
"""Indeed a similar performance eventhough having a much simpler tree.
So lets evaluate it in depth

#### Evaluate our classifiers
"""

#%%
def get_clf_performance(name,clf,x_train,y_train,x_test,y_test):
  print(name)
  print("classification report")
  y_test_pred = clf.predict(x_test)
  print(classification_report(y_test, y_test_pred))

for t in [("dct_clf",dct_clf),("dct_clf2",dct_clf2)]:
  get_clf_performance(t[0],t[1],x_train,y_train,x_test,y_test)

# %% [markdown]
"""If you compare this to our classfier in the beginning they perform similar but we managed to greatly simplify it 

However, this doesn’t really tell us anything about where we’re doing well. A useful technique for visualising performance is the confusion matrix.

This is simply a matrix whose diagonal values are true positive counts, while off-diagonal values are false positive and false negative counts for each class against the other.

OK sorry that might have confused you (pun intended ;P)

Hopefully that is more easy

anything on the diagonal was classified correctly and anything off the diagonal was classified incorrectly.

"""

#%%
def get_clf_cm(name,clf,target_names,x_train,y_train,x_test,y_test):
  
  print(name)
  cm = pd.DataFrame(
        confusion_matrix(y_test, clf.predict(x_test)),
        columns=target_names, 
        index=target_names
        )
  sns.heatmap(cm, annot=True)
  plt.xlabel("predicted")
  plt.ylabel("truth")
  plt.show()

for t in [("dct_clf",dct_clf),("dct_clf2",dct_clf2)]:
  get_clf_cm(t[0],t[1],iris.target_names,x_train,y_train,x_test,y_test)

# %% [markdown]
"""The initial (but more complex) and the simplified decission tree performed exactly the same in terms of miss classifications

#### Excercise

Apply everything you have learned on the wine dataset.
Feel free to use my mini functions to save some line of codes.
Do not stress about any time limit I just want you to get familiar
with the scikit-learn commands there is no need to finish it.
Just try to get as far as possible ;D
"""

#%%
from sklearn.datasets import load_wine

#%% [markdown]
"""##### Solution"""

#%%
from sklearn.datasets import load_wine
wine = load_wine()

#%%
x_train, x_test, y_train, y_test = train_test_split(wine.data,wine.target)

#%%
wine_clf = DecisionTreeClassifier(random_state=123456)
wine_clf.fit(x_train, y_train)

#%%
get_clf_performance("wine_clf",wine_clf,x_train,y_train,x_test,y_test)

#%%
plt.figure(figsize=(4,4), dpi=300)
plot_tree(wine_clf,feature_names=wine.feature_names,
          class_names=wine.target_names, filled=True)
plt.show()

#%%
test_df=appply_clf(wine_clf,wine,x_test,y_test)
test_df.head()

#%%
get_feature_importance(wine_clf,wine)

#%%
wine_clf2 = DecisionTreeClassifier(random_state=123456)
params = {
    "max_depth"         : [1,2,3,4,5,10],
    "criterion"         : ['entropy', 'gini'],
    "max_features"      : [0.5, 1],
    "max_leaf_nodes"    : [2,3,5],
    'min_samples_leaf'  : [2,3,5],
    'min_samples_split' : [2,3,5]
}
gs = GridSearchCV(wine_clf2, params, cv=3)
gs.fit(x_train, y_train)
print(gs.best_params_)
print()
wine_clf2=gs.best_estimator_
wine_clf2.fit(x_train,y_train)
wine_clf2

#%%
plt.figure(figsize=(4,4), dpi=300)
plot_tree(wine_clf2, feature_names=wine.feature_names, 
          class_names=wine.target_names, filled=True)
plt.show()

#%%
for t in [("wine_clf",wine_clf),("wine_clf2",wine_clf2)]:
  get_clf_performance(t[0],t[1],x_train,y_train,x_test,y_test)

#%%
for t in [("wine_clf",wine_clf),("wine_clf2",wine_clf2)]:
  get_clf_cm(t[0],t[1],wine.target_names,x_train,y_train,x_test,y_test)

# %% [markdown]
"""## Random Forest

Random Forest combines the results of many different decision trees to make the best possible decisions.

![image.png](https://upload.wikimedia.org/wikipedia/commons/7/76/Random_forest_diagram_complete.png)

#### Load Libraries
"""

#%%
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.datasets import load_iris

from sklearn.ensemble import RandomForestClassifier

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.feature_selection import SelectFromModel

from sklearn.metrics import accuracy_score,confusion_matrix,classification_report

# %% [markdown]
"""To compare it to our decision tree classifier we take again the iris dataset"""

iris = load_iris()

# %% [markdown]
"""#### Preprocess Data"""

#%%
# Split dataset into training 80% and test (20%) set
test_size=0.2
x_train, x_test, y_train, y_test = train_test_split(
    iris.data, iris.target, test_size=test_size)

for t in [("training",x_train),("test",x_test)]:
  print(f"observations in the {t[0]} set:{len(t[1])}")

# %% [markdown]
"""#### Create random forest classifier"""

num_of_random_trees=100
rf_clf = RandomForestClassifier(
    n_estimators=num_of_random_trees,
    oob_score=True, random_state=123456)

# Train the Classifier
rf_clf.fit(x_train, y_train)

# %% [markdown]
"""#### **Apply Classifier To Test Data**

"""

#%%
# View the predicted probabilities of the first 5 observations
for t in [("dct_clf2",dct_clf2),("rf_clf",rf_clf)]:
  print(t[0]);print(t[1].predict_proba(x_test)[0:5]);print()

#%%
test_df=appply_clf(rf_clf,iris,x_test,y_test)
test_df.head()

# %% [markdown]
"""Again looks pretty good! At least for the first five observations.
Lets do a quick evaluation and comparison against the decission tree results
"""

#%%
print("accuracy")
for t in [("dct_clf2",dct_clf2),("rf_clf",rf_clf)]:
  print(f'{t[0]}: {accuracy_score(y_test, t[1].predict(x_test))}')

# %% [markdown]
"""The Random forest seems to perform slightly better but before we go in detail
we again want to explore our classifier

#### Explore the classifier
"""

#%%
get_feature_importance(rf_clf,iris)

# %% [markdown]
"""No surprise for the random forest also petal length and width are the most important features. So should we then even care about the sepal length/width?
Lets find it out!

#### Improve our classifier

Of course we could also search again for the best parameters but this
time I want to show you the power of feature selection
"""

#%%
# Create a selector object that will use our classifier to identify
# features that have an importance of more than 0.15
sfm = SelectFromModel(rf_clf, threshold=0.15)

# Train the selector
sfm.fit(x_train, y_train)

for feature_list_index in sfm.get_support(indices=True):
    print(iris.feature_names[feature_list_index])

# %% [markdown]
"""Create A Data Subset With Only The Most Important Features"""

x_important_train = sfm.transform(x_train)
x_important_test = sfm.transform(x_test)

# %% [markdown]
"""Train A New Random Forest Classifier Using Only Most Important Features"""

#%%
# Create a new random forest classifier for the most important features
rf_clf2 = RandomForestClassifier(random_state=123456)

# Train the new classifier on the new dataset containing the most important features
rf_clf2.fit(x_important_train, y_train)

print("accurcacy")
for t in [("dct_clf2",dct_clf2),("rf_clf",rf_clf)]:
  print(f'{t[0]}: {accuracy_score(y_test, t[1].predict(x_test))}')

print(f'rf_clf2:{accuracy_score(y_test, rf_clf2.predict(x_important_test))}')

# %% [markdown]
"""As expected our original model which contained all four features performs
slightly better then our feature selected model. So for a small cost in accuracy we halved the number of features in the model

#### **Evaluate our classifiers**
"""

#%%
for t in [("dct_clf2",dct_clf2),("rf_clf",rf_clf)]:
  get_clf_performance(t[0],t[1],x_train,y_train,x_test,y_test)

get_clf_performance(
    "rf_clf2",rf_clf2,x_important_train,y_train,x_important_test,y_test)

# %% [markdown]
"""Performance stats look promising but better evaluate it in action!"""

#%%
for t in [("dct_clf2",dct_clf2),("rf_clf",rf_clf)]:
  get_clf_cm(t[0],t[1],iris.target_names,x_train,y_train,x_test,y_test)

get_clf_cm(
    "rf_clf2",rf_clf2,iris.target_names,
    x_important_train,y_train,x_important_test,y_test)

# %% [markdown]
"""So again only one missclassification so who knows maybe
we could improve it even further lets find out in the exercise

#### Exercise

In this exercise I just want you to play a litte with random forest classifier.
So try to find a better performing classifier.
But do not stress out I also have not found any ;D

For comparison please use random_state=123456

Hint:do not use too many trees (n_estimators)
otherwise it will run for ages :D

###### Solution
"""

#%%
rf_clf3 = RandomForestClassifier(random_state=123456,n_estimators=100)
params = {
    "max_depth": [1,2,3,4,5,10],
    "criterion": ['entropy', 'gini'],
    "max_features": [0.5, 1],
    "max_leaf_nodes": [2,3,5],
    'min_samples_leaf': [2,3,5],
    'min_samples_split': [2,3,5]
}
gs = GridSearchCV(rf_clf3, params, cv=3)
gs.fit(x_important_train, y_train)
print(gs.best_params_)
print()
rf_clf3=gs.best_estimator_
rf_clf3.fit(x_important_train,y_train)
rf_clf3

for t in [("rf_clf2",rf_clf2),("rf_clf3",rf_clf3)]:
  get_clf_performance(
      t[0],t[1],x_important_train,y_train,x_important_test,y_test)

get_clf_cm("rf_clf3",rf_clf3,iris.target_names,
           x_important_train,y_train,x_important_test,y_test)

# %% [markdown]
"""## Support Vector Machines

A support vector machine (SVM) uses algorithms to train and classify data within degrees of polarity, taking it to a degree beyond X/Y prediction. 

For a simple visual explanation, we’ll use two tags: red and green, with two data features: X and Y, then train our classifier to output an X/Y coordinate as either red or green.


# ![image.png](https://miro.medium.com/max/724/0*INqwwHXgTabQx7wM.png)

#### Load Libraries
"""

#%%
import random

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn.metrics import accuracy_score

# %% [markdown]
"""#### generate a random dataset"""

#%%
def generate_random_list(seed):
    lst=[]
    for t in seed:
      start,end,ndigits=t
      lst.append(round(random.uniform(start,end),ndigits))
    return lst

def generate_random_dataset(size):
    # taken from
    #https://towardsdatascience.com/support-vector-machines-explained-with-python-examples-cb65e8172c85
    seed_x = [(0,2.5,1),(1,5,2),(3,5,2)]
    seed_y = [(0,20,1),(20,25,2),(5,25,2)]
    x,y,target = [],[],[]
    for i in range(size):
      x.extend(generate_random_list(seed_x))
      y.extend(generate_random_list(seed_y))
      target.extend([0,1,1])

    df = pd.DataFrame(list(zip(x,y,target)),columns=['x', 'y','target'])
    return df

# Generate dataset
dataset_size = 100
df = generate_random_dataset(dataset_size)
features = df[['x', 'y']].to_numpy()
label = df['target'].to_numpy()
target_names = np.unique(label)

print("generated dataset")
df.head()

#%%
# Split dataset into training (80%) and testing (20%) set
x_train, x_test, y_train, y_test = train_test_split(
    features, label, test_size=0.2)

for t in [("training",x_train),("test",x_test)]:
  print(f"observations in the {t[0]} set:{len(t[1])}")

#%%
# Plotting the training set
fig, ax = plt.subplots(figsize=(12, 7))
ax.scatter(x_train[:,0],x_train[:,1])
plt.show()

# %% [markdown]
"""There’s a little space between the two groups of data points. But closer to the center, it’s not clear which data point belongs to which class.

#### Fit a linear SVM

A linear curve might be a good candidate to separate these classes. So let’s fit the SVM
"""

#%%
svm_lin_clf = svm.SVC(kernel='linear',random_state=123456)
svm_lin_clf.fit(x_train, y_train)

#%%
def plot_decision_boundaries(clf,features,x_train,y_train):
  fig, ax = plt.subplots(figsize=(12, 7))
  
  # Create grid to evaluate classifier
  xx = np.linspace(-1, max(features[:,0]) + 1, len(x_train))
  yy = np.linspace(0, max(features[:,1]) + 1, len(y_train))
  YY, XX = np.meshgrid(yy, xx)
  xy = np.vstack([XX.ravel(), YY.ravel()]).T
  
  # Assigning different colors to the classes
  colors = np.where(y_train == 1, '#8C7298', '#4786D1')
  
  # Plot the dataset
  ax.scatter(
      x_train[:,0],
      x_train[:,1],
      c=colors)
  
  # Get the separating hyperplane
  Z = clf.decision_function(xy).reshape(XX.shape)
  
  # Draw the decision boundary and margins
  ax.contour(XX, YY, Z, colors='k', 
             levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])
  
  # Highlight support vectors with a circle around them
  ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],
             s=100, linewidth=1, facecolors='none', edgecolors='k')
  
  print("Dataset after classification, with decision boundary (full line)")
  print("margin (dashed lines) and support vectors marked with a circle.")
  plt.show()

plot_decision_boundaries(svm_lin_clf,features,x_train,y_train)

# %% [markdown]
"""If we calculate the accuracy of this model against the testing set we get a good result, granted the dataset is very small and generated at random."""

#%%
print(f'accuracy: {accuracy_score(y_test, svm_lin_clf.predict(x_test))}')

get_clf_cm("svm_lin_clf",svm_lin_clf,target_names,x_train,y_train,x_test,y_test)

# %% [markdown]
"""#### Fit a quadratic SVM

The accuracy is good, but let's see if a more complex approach can return an even better result. To fit an SVM with a polynomial kernel we just need to update the kernel parameter.
"""

#%%
svm_poly_clf = svm.SVC(kernel='poly', degree=2,random_state=123456)
svm_poly_clf.fit(x_train, y_train)

#%%
plot_decision_boundaries(svm_poly_clf,features,x_train,y_train)

#%%
print("accuracy")
for t in [("svm_lin_clf",svm_lin_clf),("svm_poly_clf",svm_poly_clf)]:
  print(f'{t[0]}: {accuracy_score(y_test, t[1].predict(x_test))}')

# %% [markdown]
"""So it turns out that for this problem a simpler model, an SVM with a linear kernel, was the best solution."""

for t in [("svm_lin_clf",svm_lin_clf),("svm_poly_clf",svm_poly_clf)]:
  get_clf_cm(t[0],t[1],target_names,x_train,y_train,x_test,y_test)

# %% [markdown]
"""### Exercise

Ok that was easy because we only did a binary classification lets
try our best again with our beloved iris dataset
"""

#%%
iris=load_iris()

# %% [markdown]
"""###### Solution"""

#%%
x_train, x_test, y_train, y_test = train_test_split(iris.data,iris.target)

#%%
iris_clf = svm.SVC(kernel='linear')
iris_clf.fit(x_train, y_train)

#%%
test_df=appply_clf(iris_clf,iris,x_test,y_test)
test_df.head()

#%%
accuracy = accuracy_score(y_test, iris_clf.predict(x_test))
print(f'Mean accuracy score: {accuracy:.3}')

#%%
get_clf_cm("iris_clf",iris_clf,iris.target_names,x_train,y_train,x_test,y_test)

#%%
get_clf_performance("iris_clf",iris_clf,x_train,y_train,x_test,y_test)

#%%
#Bonus: Feature importance
def f_importances(coef, names):
    imp,names = zip(*sorted(zip(coef,names)))
    plt.barh(range(len(names)), imp, align='center')
    plt.yticks(range(len(names)), names)
    plt.show()

f_importances(iris_clf.coef_[0], iris.feature_names)

#%% [markdown]
"""
Note
Getting the feature importance in a non-linear SVM is impossible!
In linear SVM the resulting separating plane is in the same space as your input features. 
Therefore its coefficients can be viewed as weights of the input's "dimensions".
In other kernels, the separating plane exists in another space - a result of kernel transformation of the original space. 
Its coefficients are not directly related to the input space
"""
# %%
